%% File: trainBeamformingAgent.m
% Script to train a DQN agent for beamforming

% Ensure element pattern MAT-file exists
if ~isfile('AAAMain.mat')
    run(which('AAAMain.m'));  % populates `element`
    save('AAAMain.mat','element');
end
% Load only the pattern data
data = load('AAAMain.mat','element');
pattern = data.element;

env = BeamformingEnv(pattern);
obsInfo = getObservationInfo(env);
actInfo = getActionInfo(env);

% Build Q-network and Q-value representation
criticNet = buildCriticNetwork(obsInfo,actInfo);
qRepresentation = rlQValueRepresentation(criticNet, obsInfo, actInfo, ...
    'ObservationInputNames',{'state'}, 'OutputName','qOut');

% Epsilon-greedy exploration options
egOpts = rl.option.EpsilonGreedyExploration('Epsilon',1.0, ...
    'EpsilonMin',0.05, 'EpsilonDecay',1e-4);

agentOpts = rlDQNAgentOptions( ...
    'SampleTime',1, ...
    'DiscountFactor',0.99, ...
    'MiniBatchSize',32, ...
    'ExperienceBufferLength',1e4, ...
    'TargetUpdateFrequency',4, ...
    'EpsilonGreedyExploration',egOpts ...
);
agent = rlDQNAgent(qRepresentation, agentOpts);

% Training options
tOpts = rlTrainingOptions( ...
    'MaxEpisodes',500, ...
    'MaxStepsPerEpisode',1, ...
    'ScoreAveragingWindowLength',20, ...
    'Plots','training-progress', ...
    'Verbose',false ...
);

% Train and save
tStats = train(agent, env, tOpts);
save('BeamformingAgent.mat','agent','tStats');